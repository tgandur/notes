---
title: >-
 Use of AI in Mental Health Care Community and Mental Health Professionals
 Survey
authors: >-
 Shane Cross, Imogen Bell, Jennifer Nicholas, Lee Valentine, Shaminka
 Mangelsdorf, Simon Baker, Nick Titov, Mario Alvarez-Jimenez
year: 2024
source: JMIR Mental Health
doi: 10.2196/60589
tags:
 - dual-perspective-study
 - ai-adoption-rates
 - usage-patterns
 - ai-mental-health
 - mental-health-professionals
 - community-attitudes
type: source
status: reviewed
added: 2024-12-13T00:00:00.000Z
relevance: high
---

# Cross 2024 - AI Use Community MH Professionals Survey
#lit #ai-ethics

Related: [[202512011220-AI Ethics & Mental Health Professionals]]


**Authors:** Shane Cross, Imogen Bell, Jennifer Nicholas, Lee Valentine, Shaminka Mangelsdorf, Simon Baker, Nick Titov, Mario Alvarez-Jimenez
**Year:** 2024
**Source:** JMIR Mental Health

## Key Findings

1. **AI adoption rates**: 28% of community members (CMs) and 43% of mental health professionals (MHPs) reported using AI in the previous 6 months, with ChatGPT being the most common tool (52% CMs, 54% MHPs).

2. **Different use patterns**: CMs primarily used AI for quick emotional support (60%) and as a personal therapist/coach (47%), while MHPs used it for research (65%) and report writing (54%).

3. **Benefits and harms coexist**: While majority found AI beneficial (77% CMs, 92% MHPs), approximately half experienced specific harms and concerns (47% CMs, 51% MHPs) - including too-general responses, accuracy issues, data privacy concerns, and ethical uncertainties.

4. **Attitude differences**: MHPs had more positive attitudes toward AI (mean 6.74/10) compared to CMs (mean 5.63/10), and were more interested in future AI use for professional purposes.

5. **Mixed sentiment about future**: Equal mix of positive and negative sentiments in open feedback - optimism about accessibility and efficiency balanced by concerns about reduced human connection, ethics, privacy, medical errors, and data security.

## Methodology

**Design:** Cross-sectional web-based survey study (descriptive + thematic analysis)
**Sample:** 107 community members and 86 mental health professionals in Australia
**Data Collection:**
- Two separate web-based surveys via Qualtrics (February-April 2024)
- Snowballing recruitment via social media (LinkedIn, Instagram, Facebook) for 8 weeks
- Eligibility: aged 16+, residing in Australia
**Analysis:**
- Descriptive statistics (SPSS)
- AI Attitudes Scale (4-item, Cronbach α=0.82)
- Thematic analysis of open-ended responses
- Sentiment coding (positive/negative/neutral based on keywords and context)

## Relevance to My Research

**Fits with:** [[AI-Clinical-Psychology-Ethics/lit-review-master|Literature Review]]

**Why it matters:**
- First study to survey BOTH community members and mental health professionals on AI use patterns, experiences, and attitudes - provides dual perspective crucial for understanding implementation challenges
- Documents actual usage rates and specific applications (not just hypothetical attitudes) - shows AI is already being used despite lack of regulation
- Identifies critical gap between intended use and actual use of commercial AI tools (e.g., CMs using ChatGPT as therapist despite it not being designed for this)
- Highlights key ethical concerns from professional perspective: over-reliance on AI, lack of critical analysis of outputs, ethical uncertainty about professional use
- Demonstrates that even among users who find AI beneficial, harm experiences are common - suggests need for safeguards and education

**Turkish context:**
- Australian study, but methodology could be directly replicated in Turkey to understand Turkish MHPs' and patients' AI use patterns
- Likely similar pressures in Turkey: workforce shortages, burnout, administrative burden on clinicians - AI positioned as solution
- Turkish context may have additional concerns: language barriers (most AI tools English-dominant), cultural appropriateness of AI responses, collectivist vs individualist therapeutic approaches
- Turkey's mental health system may have different regulatory landscape - study highlights need for ethical guidelines and legislation that Turkey currently lacks
- Study demonstrates importance of surveying actual use rather than hypothetical attitudes - critical for Turkish context where digital mental health adoption patterns may differ

## Important Quotes

> "Large language model technologies like ChatGPT, are increasingly used by certain groups of consumers as an alternative to seeing a qualified MHP, and by some MHPs to assist with burdensome administrative tasks" (p. 2)

> "Respondents believe AI will offer future advantages for mental health care in terms of accessibility, cost reduction, personalization, and work efficiency. However, they were equally concerned about reducing human connection, ethics, privacy and regulation, medical errors, potential for misuse, and data security." (p. 1)

> "I have some concern that clinicians may overly rely on AI decisions or outputs that they do not critically analyse the outputs when they make clinical decisions." (MHP quote, p. 7)

> "Lack of human connection increasing the issues that harm mental health in the first place." (CM quote, p. 7)

> "AI not being able to pick up on serious distress signals that a human would notice." (CM concern, p. 7)

## Limitations

- Small sample size (107 CMs, 86 MHPs) limits generalizability
- Online recruitment via social media may oversample tech-comfortable individuals (though ~20% reported tech discomfort)
- Self-reported benefits and harms may underestimate hidden or delayed effects
- Australia-specific sample - cultural and healthcare system differences limit international generalizability
- Snowball sampling method may introduce selection bias
- Cross-sectional design - cannot track changes over time
- No detailed exploration of HOW professionals are using AI or what safeguards they employ

**What's missing for my research:**
- No exploration of professional training needs or competency frameworks for AI use
- Limited examination of ethical decision-making process when MHPs choose to use/not use AI
- No data on Turkish or non-Western contexts
- Doesn't address cultural appropriateness of AI tools or cross-cultural validity
- No analysis of professional regulatory perspectives or institutional policies
- Missing depth on specific ethical dilemmas encountered (though mentions uncertainty)

## My Notes

**Critical insights:**
1. **Gap between design and use**: Study reveals dangerous pattern - CMs using commercial AI (ChatGPT) as therapist/coach despite tools not being designed or validated for this. Half experienced harms. This unregulated use happening NOW, not hypothetical future.

2. **Professional ethical uncertainty**: 24% of MHP users uncertain about ethics of using AI for professional purposes. This uncertainty is itself a finding - suggests lack of clear guidelines, training, professional standards.

3. **Administrative burden as driver**: MHPs' top use cases all relate to time-saving (admin tasks 8.16/10, research 8.07/10) - reflects broader burnout crisis. AI positioned as solution to systemic workforce problems.

4. **Dual nature of perceived benefits**: Benefits (accessibility, 24/7 availability, cost) are real, but they don't address fundamental question - is AI-mediated support clinically appropriate/safe? Study shows benefits and harms coexist.

5. **Sentiment analysis findings crucial**: Equal positive/negative sentiment about future (not overwhelmingly optimistic or pessimistic). Professionals simultaneously excited and concerned - reflects genuine uncertainty about implications.

**For Turkish context research:**
- This methodology (dual survey of MHPs + patients/community) would be excellent model for Turkish study
- Need to explore whether Turkish MHPs experiencing similar time pressures and considering AI for administrative relief
- Critical to assess Turkish MHPs' ethical preparedness and awareness of AI risks
- Turkish professional organizations (TBD, TPD) likely haven't issued AI guidance - this study shows why urgent

**Theoretical connections:**
- Relates to technology acceptance models - but shows attitude-behavior gap (moderate attitudes but growing use)
- Professional ethics frameworks inadequate for AI era - MHPs uncertain despite professional training
- Digital divide concerns - but reversed (privileged populations early adopters, potentially experiencing harms first)

**Red flags for ethics:**
- "About half reported experiencing harms or concerns" among USERS - not general population, but people actively choosing to use AI
- CMs using AI for crisis support despite tools not designed for this - suicide risk assessment impossible for current AI
- Professional over-reliance concern raised by MHPs themselves - suggests awareness of risk but unclear how to mitigate

## Connections

- [[Zhang 2023 - MH Professionals Qualitative AI Adoption]] - qualitative complement to this quantitative survey
- [[Blease 2024 - Psychiatrists GenAI Experiences]] - psychiatrist-specific perspectives
- [[Professional competencies for AI use]]
- [[Ethical guidelines for AI in mental health]]
- [[Digital divide in mental health technology]]
- [[Administrative burden and clinician burnout]]

---

**Rating:** ⭐⭐⭐⭐⭐
**Status:** Reviewed
**Added to draft:** No
**Date processed:** 2024-12-13
