---
title: 'Use of AI in Mental Health Care: Community and Mental Health Professionals
  Survey'
authors:
- Shane Cross
- Imogen Bell
- Jennifer Nicholas
- Lee Valentine
- Shaminka Mangelsdorf
- Simon Baker
- Nick Titov
- Mario Alvarez-Jimenez
year: '2024'
source: devonthink
devonthink_uuid: A4187002-BBAE-4269-AF9B-5B122E1D26A1
devonthink_link: x-devonthink-item://A4187002-BBAE-4269-AF9B-5B122E1D26A1
url: https://mental.jmir.org/2024/1/e60589
doi: 10.2196/60589
journal: JMIR Mental Health
tags:
type: source
status: reviewed
added: '2025-12-29T00:00:00+00:00'
relevance: high
permalink: efforts/research/public-attitudes-ai-mental-health/sources/paper-notes/cross-2024-ai-mental-health-community-professionals-survey
---

# Use of AI in Mental Health Care: Community and Mental Health Professionals Survey
#lit #ai_attitudes

Related: [[202601301240-AI User Attitudes]]


**Authors:** Cross S, Bell I, Nicholas J, Valentine L, Mangelsdorf S, Baker S, Titov N, Alvarez-Jimenez M
**Year:** 2024
**Journal:** JMIR Mental Health
**PDF:** [Open in DEVONthink](x-devonthink-item://A4187002-BBAE-4269-AF9B-5B122E1D26A1)

## Key Findings

- **First study to survey BOTH community members (CMs) and mental health professionals (MHPs)** on AI use patterns, experiences, benefits and harms in mental health care
- **AI use rates:** 28% of CMs and 43% of MHPs used AI tools (mostly ChatGPT) in previous 6 months
- **Attitude divergence:** MHPs scored significantly higher on AI Attitudes Scale (6.74) vs CMs (5.63) and community norms (5.54)
- **CM use patterns:** Quick mental health advice (60%), personal therapist/coach (47%)
- **MHP use patterns:** Research (65%), report writing (54%)
- **Harms experienced:** ~50% of both groups reported concerns (too general, inaccurate, data security, ethics)
- **Commercial AI tools are being used for mental health despite NOT being designed for this purpose**

## Methodology

**Design:** Cross-sectional web-based survey study
**Sample:**
- Community Members: n=107 (Australia, 16+ years)
- Mental Health Professionals: n=86 (Australia)
**Period:** 8 weeks, February-April 2024
**Recruitment:** Social media (LinkedIn, Instagram, Facebook) snowballing
**Measures:**
- Demographics + K10 (CMs only)
- AI Attitudes Scale (AIAS-4) - 4-item, 1-10 scale, α=0.82
- AI use cases acceptability (0-10 likelihood)
- Experienced benefits, harms, concerns
- Free-text open-ended responses (thematic analysis)

## Relevance to Research

**Critical for our Turkish study because:**

1. **Dual-stakeholder methodology** - We should consider surveying both general public AND mental health professionals to understand the full picture
2. **Validated instrument** - AI Attitudes Scale (AIAS-4) used here, also mentioned in Vo 2023 as useful measure
3. **Use case endorsement patterns** - CMs prefer indirect AI support (recommendations, mood tracking) over direct support (crisis intervention, therapeutic chatbots)
4. **Harm measurement** - Provides framework for measuring experienced harms, not just perceived risks
5. **Commercial AI reality** - People ARE using ChatGPT etc. for mental health support despite these tools not being designed for it

**Comparison opportunity:** Our Turkish sample can be compared to Australian community norms (AIAS-4 = 5.54)

## Key Quotes

> "This study is to our knowledge the first to survey both CMs and MHPs on their patterns of use, experiences, and perceived benefits and harms associated with the application of AI technologies in mental health care." (p. 8)

> "Attitudes to AI between the groups varied. CMs scored similarly to published community norms on the AI Attitudes Scale" (p. 8)

> "(full scale average score 5.63, SD 2.5 vs 5.54, SD 1.78), while MHPs scored significantly higher (full scale average score 6.74, SD 2.3 vs 5.54, SD 1.78)." (p. 8)

> "AI use cases for CMs also had lower levels of endorsement than AI use cases for MHPs." (p. 8)

> "Regarding actual use, we found that AI tools, most commonly ChatGPT, were used by around a third of CMs and 40% of MHPs." (p. 8)

> "CMs tended to use these tools to obtain quick mental health advice or to receive emotional support, and nearly half used them as a personal coach or therapist, reporting the benefits being accessibility, privacy, and low-cost." (p. 8)

> "It is important to note, however, that these commercially available AI tools are not intended for such purposes, and as such, may present predictable and unpredictable risks" (p. 8)

> "About half reported experiencing harms or concerns as a result of use, noting that responses were too general, nonpersonalized, inaccurate, or unhelpful. Further, a lack of clarity regarding data security and the ethics of using AI tools in this way was also reported." (p. 8)

> "These findings align with the broader discourse in general health care delivery on the integration of AI into care, where efficiency and productivity gains must be balanced with accuracy, reliability, and ethical considerations" (p. 8)

> "As AI tools evolve, it is essential that they are developed with ethics, inclusivity, accuracy, safety and the genuine needs of end users in mind." (p. 9)

## Thematic Analysis Results

### Table 5: Community Members' Sentiment Themes

| Theme | n (%) | Description | Quote |
|-------|-------|-------------|-------|
| **POSITIVE** | 13 (20%) | | |
| Accessibility & efficiency | 12 (92%) | AI making mental health care more accessible | "AI can provide constant, instant, and affordable support for everyone who needs it." |
| Technological advancements | 10 (77%) | Excitement about AI integration | "There are so many possibilities and it can be revolutionary for mental health diagnosis and treatment." |
| Personalized care | 7 (54%) | AI personalizing treatment | "Tailored support for young people to be supported in a way that suits them." |
| **NEGATIVE** | 17 (26%) | | |
| Lack of human connection | 10 (59%) | AI can't provide empathetic interaction | "Lack of human connection increasing the issues that harm mental health in the first place." |
| Ethical & privacy concerns | 9 (53%) | Data privacy worries | "Concerned about the privacy of therapy sessions when AI is involved." |
| Misdiagnosis/lack of sensitivity | 8 (47%) | AI may misinterpret emotions | "AI not being able to pick up on serious distress signals that a human would notice." |

### Table 6: Mental Health Professionals' Sentiment Themes

| Theme | n (%) | Description | Quote |
|-------|-------|-------------|-------|
| **POSITIVE** | 12 (24%) | | |
| Technological potential | 12 (100%) | Efficiency, accessibility, quality improvement | "The potential to deliver quality, timely, relevant health care information..." |
| Technological advancements | 5 (42%) | Excitement about diagnostics, LLM co-pilots | "Big data simulations of neural processing; LLM-based mental health co-pilots for both clinicians and patients, no more bloody referral letters!" |
| **NEGATIVE** | 13 (26%) | | |
| Risks and misuse | 13 (100%) | Over-reliance by clinicians | "I have some concern that clinicians may overly rely on AI decisions or outputs that they do not critically analyse..." |
| Ethical & regulatory | 5 (38.5%) | Lack of adequate guidelines | "...there need to be enough guardrails to make it safe." |
| Data governance & security | 4 (31%) | Privacy, confidentiality issues | "Data governance will be tricky." "AI can be used and abused by companies" |

## Use Case Endorsement (Mean likelihood 0-10)

### Community Members
| Use Case | Mean (SD) |
|----------|-----------|
| Personalized recommendations | 5.72 (3.0) |
| Mood tracking | 5.44 (3.0) |
| Early detection & monitoring | 5.28 (3.2) |
| Therapeutic chatbots | 4.48 (3.3) |
| Crisis intervention support | 4.47 (3.1) |

### Mental Health Professionals
| Use Case | Mean (SD) |
|----------|-----------|
| Administrative assistance | 8.16 (2.0) |
| Literature & research analysis | 8.07 (2.2) |
| Training & simulation | 7.43 (2.5) |
| Track & guide client progress | 7.00 (2.6) |
| Personalized treatment recommendations | 6.14 (2.9) |
| Assessment & diagnosis | 6.12 (3.0) |
| Enhancing client engagement | 5.94 (3.0) |

## Strengths

- **First dual-stakeholder study** comparing CMs and MHPs directly
- **Both quantitative and qualitative data** - use case ratings + thematic analysis
- **Validated measure** - AI Attitudes Scale with published norms
- **Practical focus** - measures actual use, not just hypothetical willingness
- **Harm measurement** - goes beyond perceived risks to experienced harms
- **Open-access** - freely available for replication

## Limitations

- **Sample size:** Relatively small (n=107 CMs, n=86 MHPs)
- **Recruitment bias:** Online recruitment may attract tech-comfortable respondents
- **Geographic scope:** Australia only - may not generalize to Turkey
- **Self-report:** Actual AI use may differ from reported use
- **Time-delayed effects:** Harms may not be immediately apparent
- **Cross-sectional:** Cannot establish causality or track changes over time

## Related Papers

- [[Vo 2023 - Multi-stakeholder AI Healthcare Preferences]] - Systematic review of 105 studies, same multi-stakeholder approach
- [[Consensus 2025 - General Public Attitudes AI Mental Health]] - Synthesis supporting attitude patterns found here

## Themes

Based on content, this paper relates to:
- [x] Public attitudes toward AI in mental health
- [x] Mental health professional attitudes
- [x] AI use patterns and adoption
- [x] Experienced benefits and harms
- [x] Data privacy and ethics concerns
- [x] Human connection vs AI support
- [ ] Turkish cultural context (Australian study, but methodology applicable)

## Personal Notes

- Notumda Table 5'i özellikle çıkarmamı istedin - yukarıda tam tablo mevcut (p. 7)
- **AIAS-4 ölçeği** Türkçe'ye çevrilmiş olabilir - Satıcı et al. paper'ını kontrol et
- **CMs'ın düşük endorsement'ı** ilginç: Doğrudan destek (chatbot, kriz) daha az tercih ediliyor, dolaylı destek (öneriler, mood tracking) daha fazla
- **"Personal therapist" kullanımı %47** - insanlar ChatGPT'yi zaten terapi için kullanıyor, bu policy gap'i gösteriyor
- **Metodoloji Türkiye için uyarlanabilir:** Aynı survey yapısı, AIAS-4, use case ratings

## Follow-up Questions

1. AIAS-4'ün Türkçe validasyonu var mı? (Satıcı et al. 2023 kontrol)
2. Türkiye'de ChatGPT kullanım oranları ne? (Genel ve mental health için)
3. CM vs MHP attitude farkı Türkiye'de de olur mu? (Profesyoneller daha pozitif mi?)
4. Commercial AI'ın mental health için kullanımına yönelik Türkiye'de policy var mı?
5. "Harm experiences" Türkiye bağlamında farklı olabilir mi? (Dil, kültürel bağlam)

---
*Added on 2025-12-29 | Annotations extracted from DEVONthink*